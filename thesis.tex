\documentclass[a4paper,11pt,twoside,openany]{book}

\usepackage{tipx}
\usepackage{algo}
% \usepackage[named]{libtex/algo}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{amsthm}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{epsfig}
\usepackage{multirow}
\usepackage{amsfonts,amsmath}
\usepackage{srcltx}
\usepackage{multirow}
\usepackage{dsfont}
\usepackage{booktabs}
\usepackage[countmax]{subfloat}
\usepackage{subcaption}
\usepackage{color}
\usepackage[table]{xcolor}
\usepackage{tabularx}
\usepackage{subfig}
\usepackage{url}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{libtex/thesisStyle}
\usepackage{float}


%%%%%%%%%%%%%
\input{libtex/procs}
\input{libtex/alias}
%%%%%%%%%%%%%

\newtheorem{definition}{Definition}

\newcommand{\CAS}{\mbox{\textit{CAS}}}
\newcommand{\FAI}{\mbox{\textit{FAI}}}
\newcommand{\NULL}{\mbox{\sc null}}
\newcommand{\True}{\mbox{\texttt{True}}}
\newcommand{\False}{\mbox{\texttt{False}}}
\newcommand{\Boolean}{\mbox{boolean}}
\newcommand{\Int}{\mbox{int}}

\newcommand{\Process}{\mbox{\sc Process}}
\newcommand{\Insert}{\mbox{\sc Insert}}
\newcommand{\TreeInsert}{\mbox{\sc TreeInsert}}
\newcommand{\RPQHeapInsert}{\mbox{\sc RHeapInsert}}
\newcommand{\RPQHeapDeleteMin}{\mbox{\sc RHeapDeleteMin}}
\newcommand{\PQSAInsert}{\mbox{\sc InsertSA}}
\newcommand{\PQSADeleteMin}{\mbox{\sc DeleteMinSA}}
\newcommand{\SBInsert}{\mbox{\sc SBInsert}}
\newcommand{\MBInsert}{\mbox{\sc MBInsert}}
\newcommand{\Put}{\mbox{\sc Put}}
\newcommand{\Transform}{\mbox{\sc Transform}}
\newcommand{\Delete}{\mbox{\sc Delete}}
\newcommand{\Traverse}{\mbox{\sc Traverse}}
\newcommand{\SplitLeaf}{\mbox{\sc SplitLeaf}}
\newcommand{\DeleteMin}{\mbox{\sc DeleteMin}}
\newcommand{\GetMin}{\mbox{\sc GetMinHeap}}
\newcommand{\InitDeletePhaseHeap}{\mbox{\sc InitDeletePhaseHeap}}
\newcommand{\InitDeletePhaseSA}{\mbox{\sc InitDeletePhaseSA}}
\newcommand{\InitDeletePhase}{\mbox{\sc InitDeletePhase}}
\newcommand{\HeapDeleteMin}{\mbox{\sc DeleteMinHeap}}
\newcommand{\DeleteMinSA}{\mbox{\sc DeleteMinSA}}
\newcommand{\BufferCreation}{\mbox{\sc BufferCreation}}
\newcommand{\TreePopulation}{\mbox{\sc TreePopulation}}
\newcommand{\Prunning}{\mbox{\sc Prunning}}
\newcommand{\Refinement}{\mbox{\sc Refinement}}
\newcommand{\QueryAnswering}{\mbox{\sc QueryAnswering}}
\newcommand{\UpdateBSF}{\mbox{\sc UpdateBSF}}
\newcommand{\FindNode}{\mbox{\sc FindNode}}
\newcommand{\TotalNodes}{\mbox{\sc TotalNodes}}
\newcommand{\HelpTree}{\mbox{\sc HelpTree}}
\newcommand{\NextIndex}{\mbox{\sc NextIndex}}


\newcommand{\BC}{\mbox{$\mathit{BC}$}}
\newcommand{\PS}{\mbox{$\mathit{PS}$}}
\newcommand{\RS}{\mbox{$\mathit{RS}$}}
\newcommand{\RawData}{\mbox{$\mathit{RawData}$}}



% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}
% \renewcommand{\algorithmiccomment}[1]{{\scriptsize //#1}}


% FOR WRITING GREEK %
\usepackage[greek,american]{babel}
\usepackage[libtex/iso-8859-7]{inputenc}

\newcommand{\selg}[0]{\selectlanguage{greek}}
\newcommand{\selam}[0]{\selectlanguage{american}}

%%%%%%%%%%%%%%%%%%%%%%%%%%
%\renewcommand{\baselinestretch}{1.5}
%\textheight=23cm \textwidth=16cm \voffset -1.5cm \hoffset -12.8mm
%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\baselinestretch}{1}\small\normalsize
\textwidth=390pt \oddsidemargin = 50pt \evensidemargin = 0pt
%%%%%%%%%%%%%%%%%%%%%%%%%%

\makeindex

\thispagestyle{empty}
\pagestyle{empty}
%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%
\long\def\symbolfootnote[#1]#2
    {\begingroup
    \def\thefootnote{\fnsymbol{footnote}}\footnote[#1]{#2}
    \endgroup}

\newcommand{\workperformedat}{\symbolfootnote[0]{This work has been
    performed at the University of Crete, School of Sciences and
Engineering, Computer Science Department.}}

\newcommand{\worksupportedby}{\symbolfootnote[0]{The work has been
    supported by the Foundation for Research and Technology - Hellas (FORTH),
    Institute of Computer Science (ICS).}}

\newcommand{\thesisdate}{March 2025 }
%%%%%%%%%%%%%%%%%%%%%%%%%%
























\begin{document}
\begin{titlepage}
\begin{center}

\LARGE \textbf{DFreSh: A Lock-Free Index for Real-Time Data Series Processing}\\[0.5cm]
\LARGE \textit{Paterakis George}\\[0.5cm]

\vfill

\normalsize{
Thesis submitted in partial fulfillment of the requirements for the\\[0.30cm]

\textit{Masters' of Science degree in Computer Science and Engineering}}\\[0.30cm]

University of Crete\\
School of Sciences and Engineering\\
Computer Science Department\\
Voutes University Campus, 700 13 Heraklion, Crete, Greece\\[0.5cm]

\vfill

\Large{Thesis Advisor: Assistant Prof. \emph{Panagiota Fatourou}}\\[0.5cm]

\vfill

\end{center}

\workperformedat{} \worksupportedby{}
\end{titlepage}

\cleardoublepage




















\thispagestyle{empty}

\newcommand{\thesistitle}{DFreSh: A Lock-Free Index for Real-Time Data Series Processing}
\newcommand{\owner}{Paterakis George}
\newcommand{\firstprof}{Panagiota Fatourou}
\newcommand{\secondprof}{Professor 2}
\newcommand{\thirdprof}{Professor 3}
\newcommand{\csdchair}{Antonios Argyros}

\begin{titlepage}

\begin{center}
\textsc{University of Crete}\\
\textsc{Computer Science Department}\\
\vspace{0.4cm}
\noindent {\textbf{\thesistitle{}}}\\
\vspace{0.4cm}
\noindent Thesis submitted by\\
\textbf{\owner{}}\\
in partial fulfillment of the requirements for the\\
Masters' of Science degree in Computer Science\\
\vspace{0.4cm} THESIS APPROVAL

\vspace{0.4cm}

\begin{tabular}{rl}
\\
Author: & \underline{\phantom{123456789012345678901234567890123456789012}}\\
    & \owner{}\\
    \\
    \\
    \\
Committee approvals: & \underline{\phantom{123456789012345678901234567890123456789012}}\\
    & \firstprof{}\\
    & {\small Assistant Professor, Thesis Supervisor}\\
    \vspace{0.2cm}
    \\
    \\
& \underline{\phantom{123456789012345678901234567890123456789012}}\\
    & \secondprof{}\\
    & {\small Professor, Committee Member}\\
    \vspace{0.2cm}
    \\
    \\
& \underline{\phantom{123456789012345678901234567890123456789012}}\\
    & \thirdprof{}\\
    & {\small  Professor, Committee Member}\\
    \vspace{0.2cm}
    \\
    \\
\hspace{1.4ex}Departmental approval: & \underline{\phantom{123456789012345678901234567890123456789012}}\\
    & \csdchair{}\\
    & {\small Professor, Director of Graduate Studies}\\
\end{tabular}
\\

\vfill Heraklion, \thesisdate{}
\end{center}

\thispagestyle{empty}

\end{titlepage}

\cleardoublepage








\thispagestyle{empty}
\begin{titlepage}
\begin{center}
\bc \Large{ \textbf{TITLE}} \ec \bc {\bf\large Abstract}\\\ec
\end{center}

Abstract

In this thesis, we present DFreSh, a real-time lock-free data series index that 
achieves high performance while maintaining robustness. DFreSh builds upon FreSh,
the current state-of-the-art in lock-free data series indexing. FreSh, in turn,
is based on Refresh, a generic framework designed to enable efficient lock-freedom
in locality-aware data series indexes. Refresh provides a systematic approach
to deriving high-performing lock-free versions of other locality-aware blocking
data structures.
Despite its strengths, FreSh has a critical limitation: 
it is a static index, incapable of supporting dynamic updates after construction.
This limitation renders FreSh unsuitable for real-time environments where data
is continuously generated and needs to be inserted-integrated into the index
without the need for complete rebuilding. Addressing this shortcoming,
DFreSh introduces dynamic update capabilities, making it a robust solution
for real-time applications while retaining the performance benefits of its predecessor.
We evaluated DFreSh through extensive experiments using both synthetic
and real-world datasets. The results demonstrate that DFreSh achieves
performance comparable to FreSh, with its lightweight mechanisms for
real-time data handling upholding the key principles of performance critical
to locality-aware data structures. 
\vfill
\end{titlepage}

\cleardoublepage


























\thispagestyle{empty}

\selg

\begin{titlepage}
\begin{center}
\bc \Large{ \textbf{������}} \ec \bc {\bf\large ��������}\\\ec
\end{center}


\vfill

\end{titlepage}


\cleardoublepage

















\bc \large{\textbf{�����������}} \ec



\cleardoublepage


\begin{flushright}
\emph{
\newline \newline \newline \newline \newline \newline \newline \newline
����� ������ ���} \\
\end{flushright}

\cleardoublepage


\selam

\pagestyle{plain}
\pagenumbering{roman}
\setcounter{page}{1}
\setcounter{tocdepth}{3}

% \tableofcontents
%     \addtocontents{toc}{\contentsline {chapter}{Table of Contents}{i}}

% \listoftables
%     \addtocontents{toc}{\contentsline {chapter}{List of Tables}{iii}}

% \listoffigures
%     \addtocontents{toc}{\contentsline {chapter}{List of Figures}{v}}

% \cleardoublepage

\pagenumbering{arabic}
%\renewcommand*\thesection{\arabic{section}}




 \pagestyle{headings}
%CHAPTER_1
\chapter{Introduction}
\label{chapter:introduction}


Processing large collections of data series is critical for a wide range of applications
~\cite{DBLP:journals/sigmod/Palpanas15,DBLP:journals/dagstuhl-reports/BagnallCPZ19,Palpanas2019}.
These applications depend on efficient similarity search, which aims to retrieve the most similar
data series to a given query $Q$.
Specifically, a similarity search operation returns a subset of data series from the collection 
that are closest to $Q$  based on a defined distance metric (or similarity function).

Beyond its standalone utility, similarity search is also a key component in various machine 
learning tasks, including clustering, classification, and outlier detection. However, 
performing similarity search at scale is computationally expensive due to two main factors 
\begin{itemize}
\item The massive size of modern data series collections, which often reach terabyte or petabyte scales.
\item The high dimensionality of data series (i.e. length) that modern applications need to analyze.
\end{itemize}

To overcome these challenges, state-of-the-art data series indexes
~\cite{DBLP:journals/pvldb/EchihabiZPB18,isax2plus,wang2013data,peng2018paris,parisplus,peng2020messi,PFP21-I,PFP21-II,hercules,dumpy}
employ data series summarization. These indexes build tree-based structures where each node 
stores summarized representations of data series, enabling efficient pruning of the dataset.
By filtering out irrelevant series early, these indexes significantly reduce the number of costly 
distance computations, improving query performance.
State-of-the-art such indexes~\cite{peng2020messi,parisplus,peng2021sing} 
use iSAX summaries~\cite{isax2plus,isaxfamily},thus we call them iSAX-based indexes.  
%
Due to their efficiency and scalability, iSAX-based indexes have been widely adopted across 
diverse application domains. These include operational health monitoring in data centers, 
vehicles, and manufacturing processes; Internet of Things (IoT) data analysis; environmental 
and climate monitoring; energy consumption analysis; financial market decision-making; 
telecommunications traffic analysis; medical diagnostics; web search optimization; 
and even agriculture, where they help identify pest infestations in crops.
%
To efficiently support these real-world use cases, state-of-the-art data series indexes
~\cite{DBLP:journals/pvldb/EchihabiZPB18,isaxfamily,peng2018paris,peng2020messi,PFP21-I,PFP21-II,hercules} 
leverage modern multicore architectures to achieve parallelism. However, most existing 
approaches rely on lock-based synchronization mechanisms, which introduce blocking behavior:
if a thread holding a lock fails or is delayed, other threads waiting on the lock are forced to stall,
leading to degraded performance.
While thread failures are relatively rare, they do occur in practice, for example, due to 
software bugs in applications using the index or issues in memory management systems. 
This is especially problematic for mission-critical applications, such as nuclear plant 
monitoring or gravitational-wave detection in astrophysics, where any delay in data processing 
can have significant consequences. Such latency-sensitive applications would greatly benefit
from our proposed approach, which eliminates blocking and ensures continuous progress.
%
While locks provide synchronization, they introduce well-known performance issues, 
such as priority inversion and convoying~\cite{F04}. Additionally, depending on how they
are used, locks can limit parallelism, leading to suboptimal performance

An alternative approach is lock-free synchronization, a well-studied technique in concurrent 
tree structures~\cite{EFRB10,EFHR14,FR2018,ABF+22} and other data structures~\cite{F04,HS08,FKR18}. 
Lock-free algorithms eliminate the need for locks, ensuring that the system as a whole 
always makes progress, regardless of thread delays or failures. This property is particularly 
beneficial for highly parallel workloads, where blocking mechanisms can create bottlenecks. 
%
In this work, we introduce FreSh, a lock-free data series index that maintains the 
correctness and strong performance of state-of-the-art lock-based indexes while avoiding 
their synchronization overheads. Additionally, we present DFreSh, a dynamic version of 
FreSh that extends its capabilities to support batches of updates while simultaneously 
answering queries. This enables real-time index updates, making it well-suited for streaming
data applications.
%
The relative performance of lock-free vs. lock-based approaches depends on the execution environment.
When threads are pinned to distinct cores, algorithms with fine-grained locks perform well. 
However, in oversubscribed settings (i.e., when there are more threads than available cores),
lock-based solutions can suffer performance degradation, as threads holding locks may be
de-scheduled, blocking others. Lock-free algorithms avoid such issues but can be more complex and,
in some cases, may not always outperform lock-based approaches when delays and failures are absent.

\noindent{\bf Challenges.} 
Achieving lock-freedom in data series indexes often requires the use of a {\em helping} mechanism,
where threads are made aware of the work other threads perform and can assist them when
needed, such as during delays or load imbalances. However, conventional helping mechanisms
tend to be costly and introduce significant overhead~\cite{F04,HS08,7515610,Williams2012CCI}.
This is one of the reasons the vast majority of the software stack still relies on locks.
Ensuring lock-freedom while maintaining the good performance of existing data series indexes 
remains a significant challenge.
%
SState-of-the-art data series indexes are designed to (a) maintain data locality
and (b) minimize synchronization, aiming for high parallelism. 
For example, they often partition the data into disjoint sets and assign a distinct thread to 
handle each set's data~\cite{parisplus,PFP21-I,PFP21-II}.
This design enables parallel and independent thread operations, reducing synchronization
and communication costs. These goals are easier to achieve with locks or barriers
~\cite{peng2018paris,peng2020messi,PFP21-I,PFP21-II}.
%
However, the way helping functions in conventional lock-free data structures is inherently
incompatible with these principles. Implementing helping in such indexes without compromising
these principles is a challenge.
%
Moreover, ensuring lock-freedom while maintaining load balancing and effective data pruning
are additional hurdles. State-of-the-art data series indexes involve multiple processing phases,
each using different data structures to optimize performance. Designing lock-based versions of
these structures is feasible, but creating lock-free versions that adhere to the communication
and synchronization principles of existing indexes is more difficult.
%
To develop a generalized approach for supporting lock-freedom in data series indexes systematically,
we must first analyze the design decisions of existing state-of-the-art indexes and the performance
principles they follow. We need to establish appropriate abstractions for the data series processing
stages, as well as design principles that ensure efficiency. Achieving these goals introduces further
challenges.
%
Lastly, a significant challenge in the context of dynamic updates, is transforming
a static index, like FreSh, into a dynamic one that supports batch insertions of updates while 
maintaining the same high performance. This dynamic index must ensure that queries continue to 
return consistent and correct results, while also guaranteeing that each query retrieves all the 
data seen by previous queries, including any updates made since the last query.

\noindent
{\bf Our approach.}
We introduce FreSh, a novel static lock-free data series index, and DFreSh, 
its dynamic counterpart that efficiently addresses all of the above challenges. 

Our experimental analysis demonstrate that FreSh matches the performance of 
MESSI~\cite{PFP21-I}, the state-of-the-art concurrent data-series index, which is lock-based. 
This confirms the efficiency of the helping mechanism we employ in FreSh. In many cases, 
FreSh even outperforms MESSI by allowing for higher parallelism during the construction of the
tree index.
It's important to note that if threads crash, MESSI (and other lock-based approaches
~\cite{peng2018paris,PFP21-I,PFP21-II,hercules}) will not terminate, which is 
why we did not provide experiments for this scenario. In contrast, 
FreSh always terminates successfully and correctly.
%
To achieve FreSh, we developed ReFreSh, a generic approach that can be applied to a
range of state-of-the-art blocking indexes to provide lock-freedom without 
incurring additional costs. 
%
ReFreSh introduces the concept of locality-aware lock-freedom, which integrates
key properties of data locality, high parallelism, low synchronization costs,
and load balancing, qualities found in many existing parallel data series indexes.
None of the conventional lock-free techniques we are aware of have been designed with
these properties in mind, and our experiments show that using traditional lock-free
techniques results in a significant performance drop (Graduate Thesis).
%
ReFreSh ensures that the underlying workload and data separation of the original data series
index are preserved, thus not compromising parallelism or load balancing. It provides a mechanism
for threads to determine whether a specific workload part has been processed, performing helping
only when necessary. ReFreSh operates in two modes:
\begin{itemize}
    \item Expeditive mode with minimal synchronization costs, which avoids any helping.
    \item Standard mode, where synchronization is necessary for helpin
\end{itemize}
%
A thread initially processes its workload in expeditive mode, and helping is only performed
after completing its assigned task. After that, threads synchronize to switch to standard mode.
This approach ensures that synchronization and communication costs remain low, similar to the
original index.
%
ReFreSh can be applied to any locality-aware algorithm (Chapter~\ref{chapter:Locality-aware}) to create its
lock-free version. FreSh (Chapter~\ref{chapter:FreSh}) follows the design principles of locality-aware
iSAX-based indexes~\cite{isaxfamily,PFP21-I}, but requires replacing the original index data structures
with lock-free versions that support both expeditive and standard modes. We introduce lock-free
implementations of several concurrent data structures, including binary trees and priority queues
(Chapter~\ref{chapter:FreSh}), which provide enhanced parallelism compared to existing solutions.
%
We believe that these lock-free implementations and the ReFreSh framework can be applied to
achieve high-performance lock-free versions of various big data processing systems.
%
To enable the systematic application of ReFreSh across all processing stages of an iSAX-based index,
we introduce the abstraction of a traverse object (Chapter~\ref{chapter:traverse-object}). The traverse object
is an abstract data type that offers a generic methodology for designing iSAX-based indexes modularly,
abstracting the key operations involved in the index function. This allows us to provide a
formal and flexible approach to designing iSAX-based indexes.
Specifically, the iSAX-based index can be implemented as a sequence of operations on traverse objects. 
%
The novelties of this work go beyond the new techniques we introduce to design the
lock-free index tree; in summary:
\begin{itemize}
\item Traverse-object abstraction: Provides a mechanism to apply the ReFreSh framework repeatedly to achieve lock-free data 
    series indexes..
\item ReFreSh framework: A generic approach that can be applied on top of any locality-aware blocking data structure to make it lock-free.
\item FreSh design: first lock-free data series index.
\item FreSh implementation: Introduces new lock-free tree and traverse-object implementations to achieve lock-freedom efficiently
\item The first lock-free data series index supporting dynamic batch insertions of updates.
\item Extends the FreSh implementation to support dynamic updates, addressing the challenge of maintaining lock-freedom during concurrent insertions..
\end{itemize}
The proposed lock-free tree incorporates several novel ideas that go beyond just engineering 
improvements. Previous approaches required copying and locally updating a leaf node L upon insertion,
followed by replacing it in the shared tree, which results in poor performance. We designed a novel
algorithm for updating leaves that employs concurrent counters, announcements, handshaking, and
other techniques to improve efficiency. The expeditive-standard mode transition also posed a
challenge, as it requires synchronization when switching between modes.


\noindent{\bf Contributions.} 
Our contributions are summarized as follows.

\noindent{$\bullet$  We develop a comprehensive theoretical framework for supporting lock-freedom 
systematically in highly-efficient data series indexes. This includes introducing key concepts
such as the traverse object, locality-awareness, and locality-aware lock-freedom, as well as
identifying the desirable principles and properties for achieving optimal performance.
This framework forms the basis for ReFreSh, a generic approach that can be applied to any 
locality-aware data series algorithm to ensure lock-freedom
} 

\noindent{$\bullet$ Based on ReFreSh, we develop FreSh, the first lock-free, efficient data series
index. We present new lock-free implementations of several data structures necessary to support the
required functionality, demonstrating the feasibility and performance of a lock-free approach.}

\noindent{$\bullet$ Building on the design of FreSh, we extend it to DFreSh, a lock-free data series
index that supports dynamic batch insertions of updates. This is the first solution to maintain
lock-freedom while allowing real-time updates, ensuring that queries always return consistent results,
including data from previous updates}

\noindent{$\bullet$ Our experiments, using large synthetic and real datasets, show that FreSh
performs as well as the state-of-the-art blocking index, without sacrificing performance for
lock-freedom. In many cases, it even outperforms existing solutions. Additionally, FreSh
significantly outperforms several lock-free baselines, which rely on conventional lock-free techniques,
due to its ability to preserve locality-awareness.}. 

\noindent{$\bullet$ We introduce a theoretical framework using the traverse object, enabling the
development of locality-aware data series indexes in a modular way. This framework allows for a
systematic application of ReFreSh to transform locality-aware blocking algorithms into efficient
lock-free versions.}

\noindent{$\bullet$ TODO: ADD DFreSh contributions.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%% PRELIMINARIES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Prelemenaries}
\label{chapter:prelemenaries}


A \textbf{data series (DS)} of size (or dimensionality) \( n \) is a sequence 
of \( n \) pairs, where each pair consists of a real value and its corresponding 
dimension.
% 
The \textbf{Piecewise Aggregate Approximation (PAA)}~\cite{DBLP:journals/kais/KeoghCPM01} 
provides a compact representation of a data series by dividing the x-axis into 
\( w \) equal segments. Each segment is then represented by the \textbf{mean value} 
of the corresponding points, as shown by the black horizontal lines in 
Figure~\ref{fig:PAA}. 
% 
To compute the \textbf{iSAX summary}~\cite{shieh2008sax}, the y-axis is partitioned 
into \( c \) discrete regions, where \( c \) represents the \textbf{cardinality} 
of the representation.
%
Each region is assigned a unique bit pattern, and instead of storing the raw PAA values, 
iSAX encodes each segment using the bit representation of the region it falls into. 
This forms a \textbf{symbolic representation} of the series, 
such as the word \( 10_2 00_2 11_2 \) in Figure~\ref{fig:iSAXSummary} 
(where subscripts indicate the number of bits used per segment).
% 
The number of bits used per region can vary, enabling the construction of a 
\textbf{hierarchical tree index}, known as an \textbf{iSAX-based tree index}, as 
illustrated in Figure~\ref{fig:iSAXTree}.
The index is a \textbf{leaf-oriented tree}, where each leaf stores up to \( M \) keys.
% 
During insertion, if the target leaf \( \ell \) has available space, the new key 
is simply added. However, if \( \ell \) is full, it undergoes a \textbf{split operation},
where it is replaced by a small subtree consisting of an \textbf{internal node} and two 
new leaves, which inherit  the keys from \( \ell \). If one of the newly created leaves
remains empty, the splitting process continues recursively.
For further details on iSAX-based indexes, refer to~\cite{isaxfamily}.


\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/prelem/timeseries.png}
        \caption{Data Series}
        \label{fig:dataseries}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/prelem/PAA.png}
        \caption{PAA Summary}
        \label{fig:PAA}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/prelem/isax.png}
        \caption{iSAX Summary}
        \label{fig:iSAXSummary}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/prelem/isaxTreeCustom.png}
        \caption{iSAX Tree}
        \label{fig:iSAXTree}
    \end{subfigure}
    
    \caption{From data series to iSAX index}
    \label{fig:from_ds_to_iSAX}
\end{figure}

\noindent{\bf Similarity Search}  
We focus on \textit{exact similarity search} (also known as exact \textit{1-NN}),  
which retrieves the data series from a collection that is most similar to a given 
query series. Similarity is typically measured using \textbf{Euclidean Distance (ED)},
but our techniques are general enough to support other widely used 
\textit{similarity measures}, such as Dynamic Time Warping (DTW)~\cite{rakthanmanon2012searching}.  
% 
The \textbf{Euclidean distance} between two time series  
\( T = \{t_1, ... , t_n\} \) and \( T' = \{t'_1, ... , t'_n\} \)  
is defined as:  
\[
ED(T, T') = \sqrt{\sum_{i=1}^{n} (t_i - t'_i)^2}
\]  
% 
We refer to the distance between the \textit{iSAX summaries} of two data series  
as the \textbf{lower-bound distance}.  
The calculation of this distance guarantees the \textbf{pruning property}:  
the lower-bound distance between two data series is always less than or equal to  
their Euclidean distance, which we refer to as the \textbf{real distance}.  
% 
This property enables efficient pruning of candidates during query processing:  
a data series can be \textbf{pruned} if its lower-bound distance to the query series
\( Q \) is greater than the real distance of any other data series in the collection 
from \( Q \).

\noindent
{\bf Leaf-Oriented Trees.}  
In a \textit{leaf-oriented tree}, all data are stored in the leaves, with each leaf
capable of holding up to \( M \) keys.  
% 
During an insertion, if the target leaf \( \ell \) has available space,  
the new key is simply added to \( \ell \).  
However, if \( \ell \) is full, it undergoes a \textbf{split}: it is replaced 
by a subtree consisting of an internal node and two new leaves.  
The keys from \( \ell \) are then redistributed between the new leaves based on
their values.  
% 
If one of the newly created leaves remains empty after redistribution,  
the splitting process is repeated until both leaves contain keys.  

\noindent\textbf{iSAX-Based Indexing.}
Concurrent iSAX-based indexes~\cite{peng2018paris,parisplus,peng2020messi,  
PFP21-I,PFP21-II} operate in two main phases:  
the \textit{tree index construction phase} and the \textit{query answering phase},  
each utilizing distinct data structures. 
During the \textbf{tree index construction phase}, a set of \textit{worker threads}  
processes a collection of input data series (i.e., \textit{raw data}).  
Each series is summarized using an iSAX representation and inserted into a 
\textit{tree index} as a pair of an iSAX summary and a pointer to the corresponding 
data series.  
% 
The process is divided into two main stages:
\begin{itemize}
    \item \textbf{Buffers Creation:} iSAX summary pairs are first stored in array-based  
    \textit{summarization buffers}.  
    \item \textbf{Tree Population:} Worker threads traverse these buffers and insert 
    their entries into the tree index.  
\end{itemize}  
Data series with similar iSAX representations are placed in the same buffer and later  
within the same subtree of the index tree.  
This approach ensures \textbf{high parallelism}, \textbf{good locality}, and  
\textbf{low synchronization overhead} during index construction.

\bigskip  
\noindent\textbf{Query Answering.}  
Given a query data series \( Q \), the system follows these steps:  
\begin{enumerate}
    \item The iSAX summary of \( Q \) is computed and used to traverse the index tree,  
    leading to a leaf \( \ell \).  
    \item The \textit{real distance} between \( Q \) and each data series in \( \ell \) is computed.  
    \item The smallest distance found so far is stored in the \textbf{Best-So-Far (BSF)} variable,  
    serving as an initial approximate answer.  
\end{enumerate}  
Query answering proceeds in two stages:  
\begin{itemize}
    \item \textbf{Pruning Stage:}  
    \begin{itemize}
        \item A set of \textit{query answering threads} traverse the tree.  
        \item If the \textit{lower bound distance} from a node to \( Q \) exceeds BSF,  
        the node is \textit{pruned} and ignored in further processing.  
        \item This guarantees that no pruned data series can be the final answer.  
    \end{itemize}
    \item \textbf{Refinement Stage:}  
    \begin{itemize}
        \item Candidate series are stored in one or more \textit{priority queues}~\cite{parisplus,PFP21-I,PFP21-II}.  
        \item Multiple threads process these candidates, calculating their exact distances to \( Q \).  
        \item BSF is updated whenever a smaller distance is found.  
    \end{itemize}
\end{itemize}   
At the end of this phase, the final answer is contained in BSF.  
\textit{Barriers} synchronize threads at the end of each stage, ensuring correctness,  
while \textit{locks} handle concurrent access to shared data structures.  
Figures~\ref{fig:example} and~\ref{fig:example2} summarize the indexing process.  

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/prelem/iSAX-index.pdf}
    \caption{Similarity search with the use of a data series index.}
    \label{fig:example}
    \vspace{-0.5cm} % Reduces space after the figure
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/prelem/flowchart2.pdf}
    \caption{Index building and query answering flowchart for the MESSI data series index.}
    \label{fig:example2}
    \vspace{-0.5cm} % Reduces space after the figure
\end{figure}


\noindent{\bf MESSI as an example.} 
MESSI~\cite{peng2020messi} is a state-of-the-art, in-memory iSAX-based index.
It uses an array, referred to as \textit{RawData}, to store the raw data. During the 
{\em buffers creation stage}, this array is split into a number of fixed-size chunks 
containing consecutive raw data series. Worker threads repeatedly {\em acquire} and
process these chunks, storing the calculated iSAX summaries in the appropriate 
summarization buffers. Threads determine which chunks to work on using a \FAI\ object.
Each thread is allocated its own space in each summary buffer to avoid collisions when
adding elements, ensuring thread safety. This process continues until all data series in
\textit{RawData} have been processed.
% 
During the {\em tree population stage}, worker threads again use \FAI\ to {\em acquire}
iSAX buffers to work on. Each subtree of the index tree is a binary leaf-oriented tree
with fat leaves.
% 
In the {\em query answering phase}, a query answering worker repeatedly {\em acquires}
a subtree (using \FAI) and {\em traverses} it by calculating the {\em lower-bound
distance} between the query series \( Q \) and the iSAX summary of each node
encountered. If the lower-bound distance of a leaf is smaller than the Best So Far (BSF)
distance, the leaf is inserted into a set of {\em priority queues}, with the distance
used as the priority. Threads insert elements into these queues in a round-robin fashion.
Since a priority queue may be concurrently accessed by multiple threads, 
MESSI employs a coarse-grain {\em lock} for synchronization.
% 
During the {\em refinement phase}, each query answering thread \( t \) is assigned a 
priority queue \( \mathit{PQ} \) to process. It repeatedly removes the leaf with the
minimum priority from \( \mathit{PQ} \) and compares its iSAX summary to the BSF.
If the leaf's summary is smaller, the real distances between the data series stored
in the leaf and the query series are computed. Otherwise, the leaf and all remaining
nodes in the queue are pruned. Since multiple threads may process a priority queue
concurrently, it is protected by a coarse-grain lock. Once processing of a priority
queue is complete, thread \( t \) moves on to the next priority queue in a round-robin
manner.
% 
{\em Barriers} are used among threads at the end of each stage and before the start of 
the next to ensure correct synchronization and maintain the integrity of the process.

\noindent{\bf System.}  
We consider a shared-memory system with \( N \) threads that execute {\em concurrently
and asynchronously} while communicating by accessing shared objects. A shared object
\( O \) can be atomically read or written. Additionally, the operation \FAI(O, v)
atomically reads the current value of \( O \), adds the value \( v \) to it, and
returns the value that was read. The operation \CAS($O, u, v$) reads the value of
\( O \), and if it is equal to \( u \), it changes it to \( v \) and returns
\textit{True}; otherwise, \( O \) remains unchanged and \textit{False} is returned.
% 
Threads may experience delays (e.g., due to page faults, power consumption issues,
or overheating~\cite{inteloverheating}), or they may fail by crashing (e.g., due to
software errors). An algorithm is said to be {\em blocking} if a thread must wait for
actions to be performed by other threads in order to make progress. {\em Lock-freedom}
guarantees that the system as a whole continues to make progress, independently of the
speed of threads or their failures.


\subsection{Other Related Work}
Numerous tree-based techniques for efficient and scalable data series similarity search
have been proposed~\cite{DBLP:journals/pvldb/EchihabiZPB18, DBLP:journals/pvldb/EchihabiZPB19,
DBLP:conf/edbt/EchihabiZP21, DBLP:journals/pvldb/EchihabiPZ21},
including approximate~\cite{DBLP:journals/pvldb/AziziEP23, 
DBLP:journals/kais/LevchenkoKYAMPS21} and 
progressive~\cite{DBLP:conf/sigmod/GogolouTEBP20, DBLP:journals/tvcg/JoSF20,
DBLP:conf/sigmod/LiZAH20, DBLP:journals/vldb/EchihabiTGBP23} solutions.
Among these, iSAX-based indexes~\cite{isaxfamily} have proven to be particularly
competitive in terms of both index construction and query performance~\cite{DBLP:journals/pvldb/EchihabiZPB18,
DBLP:journals/pvldb/EchihabiZPB19, hercules, odyssey, dumpy}. These indexes also include
parallel and distributed solutions that leverage modern hardware (e.g., SIMD, multi-core,
multi-socket, GPU), such as ParIS+~\cite{parisplus}, MESSI~\cite{PFP21-I}, and SING~\cite{PFP21-II},
as well as distributed approaches like DPiSAX~\cite{dpisax, dpisaxjournal} and
Odyssey~\cite{odyssey}.
% 
The first lock-free concurrent search tree implementation was proposed in~\cite{EFRB10}.
Building on the ideas from that paper, we develop a baseline algorithm, which we discuss
and compare experimentally with \textit{FreSh} in Section~\ref{chapter:Evaluation}.
Several other non-blocking concurrent search trees have been introduced in the 
literature~\cite{BER14, HL16, ABF20, HJ12, NRM20, CNT14, BP12, EFHR14, FR2018, ABF+22}.
The key novelty of our tree implementation, presented in chapter~\ref{chapter:FreSh}, is its
ability to concurrently perform multiple insert operations in a lock-free manner to
update the array in a (fat) leaf. Additionally, it supports the expeditive-standard mode
of execution. These advancements result in improved parallelism and better performance.
Our approach focuses solely on the functionality required for implementing traversal objects,
whereas the aforementioned implementations support a variety of other features or are designed
for different contexts.
% 
Concurrent priority queues have been explored in~\cite{AK15-I, RT21, WG15, SUNDELL2005609,
tamir_et_al, LJ13}, none of which are based on sorted arrays or support different
execution modes. In our baseline lock-free implementations, we use a skip-list-based
priority queue~\cite{LJ13}, which has been shown to perform well. However, our experiments
indicate that the priority queue design we implemented for \textit{FreSh} significantly
outperforms this approach (Chapter~\ref{chapter:Evaluation}).
% 
Universal constructions~\cite{FK11spaa, FK12ppopp, FK14, FK17opodis, FK09, FK20,
FKK18, EF+16, FKK22} can provide wait-free or non-blocking concurrent versions of any
sequential data structure. However, due to their general nature, they are often less
efficient than implementations tailored to specific data structures. The algorithms
in~\cite{FK11spaa, FK14, FKK22} are highly efficient for small shared objects
(e.g., stacks and queues) but are not suitable for our application.
% 
The concept of transforming algorithms to achieve different progress guarantees is not
new, as seen in~\cite{SP14, ELM05, GKK06}, though these transformations address different
problems. The technique used in \textit{FreSh}, called \textit{ReFreSh}  departs from all these
prior approaches.

\subsection{Dynamic Data}  
In this Master's thesis, we assume that all data is initially stored in memory, but
only a small subset is used to create the initial index. After the index is built,
new data arrives dynamically in batches, with the batch size configurable by the user.
To simulate real-time data ingestion, we introduce a delay interval between consecutive batches.
This delay, which is also defined by the user, begins as soon as a batch arrives.
% 
The behavior of the system depends on the interplay between batch size and delay interval.
If the delay is long enough, index workers (i.e., threads responsible for building the index)
may fully integrate each batch into the index before the next batch arrives, remaining idle for
the rest of the time. Conversely, if the delay is short or the batch size is large, new batches
may arrive before previous ones have been fully processed, leading to overlapping updates and
concurrent processing.  
% 
While index workers are responsible for inserting new data, query workers (i.e., threads
responsible for answering queries) simultaneously access the same data structures to perform
exact similarity search. Ensuring correctness in such a system requires a robust mechanism to
manage concurrent updates and queries. To achieve this, we employ two models inspired by different
domains of computer science: a \textit{timestamp-based model} and a \textit{consistency model based
on linearizability}.  

\subsubsection{Timestamp-Based Ordering}  

A well-known approach for handling dynamic data is the use of timestamps to enforce ordering 
constraints. This method is widely used in database systems to ensure that data is processed
in a meaningful sequence~\cite{babcock2002}. Notable systems employing timestamp-based mechanisms include
Apache Kafka, Google Spanner~\cite{kafka2021,spanner2013}.  
% 
In our approach, timestamps are implicitly generated using the system's clock. Specifically,
every batch is assigned a timestamp corresponding to the time when the processing of the batch starts.
This timestamp plays a crucial role in ensuring query consistency:  

\begin{itemize}  
    \item Queries must see all data that previous queries have seen.  
    \item If a query has a later timestamp than an ongoing batch, it must wait for that batch to
     finish processing before continuing.  
\end{itemize}  
% 
By enforcing this ordering, we ensure that queries always operate on a consistent view of the data.
Our approach is inspired by the principles outlined in \textit{Models and Issues in Data Stream
Systems}~\cite{babcock2002}, which guided the design and implementation of timestamps in our dynamic system.  

\subsubsection{Ensuring Correctness Through Linearizability}  

The second approach we employ is inspired by \textit{linearizability}, a key concept in concurrent
and distributed systems. Linearizability provides a strong consistency guarantee by ensuring
that all operations appear to execute atomically in a single, consistent order.  

Formally, a system is \textbf{linearizable} if:  
\begin{itemize}  
    \item There exists a total order of operations that respects their real-time execution order.  
    \item Once an operation completes, all subsequent operations observe its effects immediately.  
\end{itemize} 
% 
By leveraging the principles of linearizability, we ensure that concurrent updates and queries
interact correctly, preserving system correctness while maintaining high concurrency. 

\subsubsection{Relating Timestamp and Linearizability Models}

The Timestamp and Linearizability models complement each other by addressing different aspects
of correctness in a dynamic, concurrent system. The timestamp model ensures that data is processed
in a meaningful order, guaranteeing that queries see all data that previous queries have seen,
respecting the temporal sequence of batch arrivals and processing. However, it does not, by itself,
ensure that updates and queries across different workers maintain a consistent global order.
% 
On the other hand, linearizability ensures that all operations, whether updates or queries, are
executed atomically and observed in a consistent, real-time order across all threads and workers.
It guarantees that once an operation completes, its effects are immediately visible to all subsequent
operations, preserving consistency even in highly concurrent environments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Traverse Object %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Traverse Object}
\label{chapter:traverse-object}

\section{Traverse Object}

In this section, we introduce the \textbf{traverse object}, an abstract data type
that enables a modular design for iSAX-based indexes. The processing pipeline of
an iSAX-based index consists of multiple stages, where each stage processes data
produced by the previous one. The first stage handles the original dataset, while
subsequent stages refine and structure the data progressively. This structured
processing inspired the definition of the \textbf{traverse object}, whose
sequential specification is given below.

\subsection{Definition of the Traverse Object}

\begin{definition}
\label{def:traverse}
Let $U$ be a universe of elements. A \textbf{traverse object} $S$ stores elements
of $U$ (which may not be distinct) and supports the following operations:

\begin{itemize}
    \item \textit{Put}($S,e,\mathit{param}$): Adds an element $e \in U$ to $S$.
    The optional parameter $\mathit{param}$ allows implementations to pass
    additional arguments.
    
    \item \textit{Traverse}($S,f,\mathit{fparam},del$): Traverses $S$, applying
    the function $f$ with parameters $\mathit{fparam}$ to each element. If the
    $del$ flag is set, the elements are deleted from $S$ after being processed.
    The parameter $\mathit{fparam}$ serves the same purpose as in \textit{Put}.
\end{itemize}

A traverse object $S$ satisfies the \textbf{traversing property}:  
Each instance of \textit{Traverse} in a sequential execution applies $f$ at
least once to all distinct elements added to $S$ that have not been deleted
in a previous invocation of \textit{Traverse}.
\end{definition}

\subsection{Traverse Objects in iSAX-Based Indexing}

\begin{algorithm}[htbp]
    \footnotesize
    \vspace*{2mm}
    
    \begin{algorithmic}[1]
    
    \Procedure{InitializeSharedObjects}{}
        \State \BC $\gets$ initially contains all raw data series
        \State \TP, \PS, \RS $\gets$ initially empty
        \State $\mathit{BSF} \gets$ some initial value
    \EndProcedure
    
    \vspace*{1mm}
    \State{Code for thread $t_i$, $i \in \{0, \ldots, n-1\}$:}		
    \vspace*{1mm}

    \Procedure{\QueryAnswering(QuerySeriesSet $\mathit{SQ}$) \textbf{returns} int}{}
        \State \BC.\Traverse(\&BufferCreation(), $\mathit{BCParam}$, \False)
        \State \TP.\Traverse(\&TreePopulation(), $\mathit{TPParam}$, \False)
        \State \PS.\Traverse(\&Prunning(), $\mathit{PSParam}$, \False)
        \State \RS.\Traverse(\&Refinement(), $\mathit{RSParam}$, \True)
        \State \Return $\mathit{BSF}$
    \EndProcedure
    
    \vspace*{1mm}
    \Procedure{\BufferCreation(DataSeries $\mathit{ds}$)}{}
        \State iSAXSummary $\mathit{iSAX} \gets$ Calculate the iSAX summary for $\mathit{ds}$
        \State Index $\mathit{bind} \gets$ index to appropriate buffer based on $\mathit{iSAX}$
        \State \TP.\Put($\langle \mathit{iSAX}$, index of $\mathit{ds}$ $\rangle$, $\mathit{bind}$)
    \EndProcedure
    
    \vspace*{1mm}
    \Procedure{\TreePopulation(Summary $\mathit{iSAX}$, Index $\mathit{ind}$,
         Index $\mathit{bind}$, Boolean $\mathit{flag}$)}{}
        \State \PS.\Put($\langle \mathit{iSAX}$, $\mathit{ind} \rangle$, $\mathit{bind}$, $\mathit{flag}$)
    \EndProcedure
    
    \vspace*{1mm}
    \Procedure{\Prunning(DataSeries $Q$, DataSeriesSet $\mathit{E}$, Boolean
        $\mathit{flag}$) \textbf{returns} boolean}{}
            \State iSAXSummary $\mathit{iSAX} \gets$ Calculate the iSAX summary for $\mathit{E}$
            \State int $\mathit{lbDist} \gets$ lower bound distance between $\mathit{iSAX}$ and $Q$
            \If{$\mathit{lbDist} < \mathit{BSF}$}
                \State \RS.\Put($\langle \mathit{E, iSAX} \rangle$, $\mathit{flag}$)
                \Return \True
            \EndIf
            \Return \False
    \EndProcedure
    
    \vspace*{1mm}
    \Procedure{\Refinement(DataSeries $Q$, DataSeriesSet $E$, Summary $\mathit{iSAX}$,
     Function *\UpdateBSF) \textbf{returns} Boolean}{}
        \State int $\mathit{lbDist}$, $\mathit{rDist}$
        \State $\mathit{lbDist} \gets$ lower bound distance between $\mathit{iSAX}$ and $Q$
        \If{$\mathit{lbDist} < \mathit{BSF}$}
            \ForAll{pair $\langle \mathit{iSAX_{ds}, ind_{ds}} \rangle$ in $E$}
                \State $\mathit{lbDist} \gets$ lower bound distance between $\mathit{iSAX_{ds}}$ and $Q$
                \If{$\mathit{lbDist} < \mathit{BSF}$}
                    \State $\mathit{rDist} \gets$ real distance between $\mathit{ds}$ and $Q$
                    \If{$\mathit{rDist} < \mathit{BSF}$}
                        \State *\UpdateBSF($\mathit{BSF},\mathit{rDist}$) \Comment{user-provided routine}
                    \EndIf
                \EndIf
            \EndFor
            \Return \True
        \Else
            \Return \False
        \EndIf
    \EndProcedure
    
    \end{algorithmic}
    
    \caption{Implementation of an iSAX-based index using the traverse objects \BC, \TP, \PS, \RS.}
    \label{alg:iSAXTraverse}
    \end{algorithm}
    
    


To implement the different stages of an iSAX-based index, we use four instances of a traverse object 
to store and manipulate different types of data:

\begin{itemize}
    \item \textbf{Buffer Creation ($BC$)}: Stores the raw data series.
    \item \textbf{Tree Population ($TP$)}: Stores pairs of iSAX summaries and
     pointers to data series.
    \item \textbf{Pruning ($PS$)}: Organizes these pairs into sets corresponding
     to the leaf nodes of the tree.
    \item \textbf{Refinement ($RS$)}: Maintains priority queues of candidate
     series for query answering.
\end{itemize}

Each of these objects plays a crucial role in the indexing and query answering
process. The \textbf{buffer creation} phase utilizes an array, \RawData, to 
store the raw data series. Thus, elements of $BC$ are stored in \RawData. 
The \textbf{tree population} phase employs a set of \textbf{summarization buffers},
where iSAX summaries and pointers are initially stored. The traverse object $TP$
holds these pairs. The \textbf{pruning} stage organizes these pairs within a
leaf-oriented tree, where $PS$ manages sets of pairs corresponding to each tree leaf. 
Finally, the \textbf{refinement} phase employs priority queues to manage tree
leaves containing candidate series for further evaluation.

\subsection{Query Answering as a Sequence of Traversals}

Query answering in an iSAX-based index follows a structured sequence of four
\textit{Traverse} operations, applied in order to the different traverse objects.  
Algorithm~\ref{alg:iSAXTraverse} presents the pseudocode for implementing an iSAX-based index
using traverse objects.

Since the four stages do not overlap, their execution is typically synchronized
using barriers. In Algorithm~\ref{alg:iSAXTraverse}, these barriers (as well as any 
multithreading aspects) are embedded within the implementations of \textit{Put}
and \textit{Traverse}. This ensures that an iSAX-based index satisfies the
following key property:

\begin{definition}[\textbf{Non-Overlapping Property}]
\label{def:non-overlapping}
In every concurrent execution of the index, for every traverse object $S$, an
instance of \textit{Traverse} on $S$ is performed only after all \textit{Put}
operations that add distinct elements to $S$ have completed.
\end{definition}

\subsection{Ensuring Correctness Through Traversing and Non-Overlapping Properties}

Assuming that the \textbf{non-overlapping property} holds for $BC$, $TP$, $PS$,
and $RS$, and that \RawData initially contains all data series in the collection,
the \textbf{traversing property} guarantees that:

\begin{enumerate}
    \item The \BufferCreation function is invoked at least once for each data
     series in \RawData.
    \item The corresponding iSAX summary is computed, ensuring that at least one
     appropriate pair is inserted into $TP$.
    \item By the \textbf{non-overlapping property}, the \TreePopulation function
     processes all these pairs, inserting them into $PS$ (the index tree).
    \item The \Prunning function is applied to all elements in $PS$, and leaves
     that cannot be pruned are inserted into $RS$.
    \item The \Refinement function is applied to every traversed element of $RS$.
     Since \textit{Traverse} on $RS$ is invoked with $del = \mathit{True}$,
    priority queues can be used to implement $RS$, and \DeleteMin can be employed
    to remove elements as they are processed.
\end{enumerate}

Thus, the system ensures that only relevant candidates are further processed,
refining the query results by computing exact distances and updating the
\textbf{best-so-far} ($BSF$) value when necessary.

Implementations of \textit{Put} and \textit{Traverse} for $BC$, $TP$, $PS$, and
$RS$ in FreSh are provided in chapter~\ref{chapter:FreSh}.

\section{Dynamic iSAX-based Index}
To enable an iSAX-based index to support dynamic data processing, the traverse object
requires adaptation. In a dynamic setting, threads work concurrently and independently
on the two distinct phases of the index. Consequently, the four stages of an iSAX-based
index, and thus the traverse objects, are now distributed among the workers. \BC\ and \TP\
are assigned to the index worker's threads dedicated to the creation of the index while 
\PS\ and \RS\ are assigned to the query workers, which are threads dedicated to answering
queries.
% 
This concurrency between the two phases impacts the procedure for answering queries using
traverse objects (see Figure \ref{}). The procedure is no longer a
sequence of four invocations of \Traverse\ on different traverse objects, but is reduced
to a sequence involving only the last two traverse objects, \PS\ and \RS. This change affects
the non-overlapping property \ref{}, as it can now only be applied to the
\Traverse\ objects of the same phase (\BC, \TP\ and \PS, \RS), because \Traverse\ and
\Put\ can occur concurrently during the Index Creation and Query Answering phases.
% 
Due to the dynamic data processing, some of the data structures that implement the traverse
objects, such as \BC\ and \PS, need re-initialization before processing the next part of the
data (see Algorithm \ref{}). This is necessary because in a
dynamic setting, index workers performing \Traverse\ on \TP\ need to know if the data there
is obsolete. Query workers have the same requirement while traversing \PS.
% 
In our approach, the data structure (specifically, the summarization buffers) is accompanied
by a sequence number. During re-initialization, each index worker first resets its size to 0,
then increments its sequence number by one. This allows an index worker performing
\Traverse\ on \TP\ to determine if the data is obsolete by checking the size and sequence number
(see Algorithm line \ref{}).
% 
Additionally, to support dynamic processing, both types of workers need to perform certain
preparations. In lines \ref{}-
\ref{}, index workers repeatedly execute the
sequence of traverse objects \BC\ and \TP, and then re-initialize the traverse object \BC.
To simulate a setting where data arrives dynamically, workers wait when no data is available,
as shown in line \ref{}.

TODO: Describe the two models, SYSTEM AND LOGICAL TIMESTAMPS. ReWrite the PSEYDOCODE
% On the other hand, query workers are responsible for fixing the system's sequence number,
% which is crucial for maintaining the system's correctness (more details in Section \ref{sec:dynamic}).
% Similar to index workers, query workers repeatedly perform the sequence of traverse objects
% \PS\ and \RS\ for each query and finally re-initialize the traverse object \PS\ 
% (Algorithm \ref{alg:DynamiciSAXTraverse} Lines \ref{alg:DynamiciSAXTraverse:Query-Start}-
% \ref{alg:DynamiciSAXTraverse:Query-End}). Additionally, query workers are responsible for
% simulating the arrival of incoming data (line \ref{alg:DynamiciSAXTraverse:nextUpdate}).


\chapter{Locality-aware}
\label{chapter:Locality-aware}

\chapter{FreSh}
\label{chapter:FreSh}

\chapter{Evaluation}
\label{chapter:Evaluation}





% %BIB
\bibliographystyle{plain}
\cleardoublepage
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{bib/thesis}



\end{document}
